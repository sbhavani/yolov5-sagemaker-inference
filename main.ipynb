{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eecba9e6",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "347f89a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import boto3\n",
    "import os\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2ec488d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = Session()\n",
    "region = sess.boto_region_name\n",
    "bucket = sess.default_bucket()\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dfdbf7",
   "metadata": {},
   "source": [
    "## Load model from Torch Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b73fb92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3204b37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b9728ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.jit.load('yolov5.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6645de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ec2-user/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2022-4-8 torch 1.7.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.0 required by YOLOv5, but Python 3.6.13 is currently installed\n"
     ]
    }
   ],
   "source": [
    "model_name = \"YoloV5s\"\n",
    "model = torch.hub.load('ultralytics/yolov5', f'{model_name.lower()}')  # or yolov5m, yolov5l, yolov5x, custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5413430",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.zeros([1, 3, 640, 640], dtype=torch.float32)\n",
    "model.train()\n",
    "model_trace = torch.jit.trace(model, image)\n",
    "# Save your model\n",
    "model_trace.save('model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d294c1c",
   "metadata": {},
   "source": [
    "### Create inference script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a816682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6d2b4792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/inference.py\n",
    "import torch\n",
    "import os\n",
    "import logging\n",
    "import io\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "IMAGE_SIZE = 640\n",
    "\n",
    "# def predict_fn(image, model):\n",
    "#     return model(image)\n",
    "\n",
    "def input_fn(request_body, content_type):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    iobytes = io.BytesIO(request_body)\n",
    "    img = Image.open(iobytes)\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "    input_tensor = preprocess(img)\n",
    "    input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "    \n",
    "    return input_batch.to(device)    \n",
    "\n",
    "# postprocess\n",
    "def output_fn(predictions, content_type):\n",
    "#     return predictions.numpy().xyxy[0].to_json(orient=\"records\")\n",
    "    res = predictions.cpu().numpy().tolist()\n",
    "    rv = json.dumps(res)\n",
    "    return rv[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8a51b1",
   "metadata": {},
   "source": [
    "### Create model archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f7c32dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_archive_name = 'model.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6b4cc226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.pth\n",
      "code/\n",
      "code/.ipynb_checkpoints/\n",
      "code/.ipynb_checkpoints/inference-checkpoint.py\n",
      "code/inference.py\n"
     ]
    }
   ],
   "source": [
    "!tar -cvzf {model_archive_name} model.pth code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ee34bd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model uploaded to: s3://sagemaker-us-east-2-156991241640/model/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# model package tarball (model artifact + inference code)\n",
    "model_url = sess.upload_data(path=model_archive_name, key_prefix='model')\n",
    "print('model uploaded to: {}'.format(model_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a477ed63",
   "metadata": {},
   "source": [
    "### Create model and test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f2c9f893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "framework_version = '1.7.1'\n",
    "py_version = 'py36'\n",
    "\n",
    "sm_model = PyTorchModel(model_data=model_url,\n",
    "                               framework_version=framework_version,\n",
    "                               role=role,\n",
    "                               sagemaker_session=sess,\n",
    "                               entry_point='code/inference.py',\n",
    "                               py_version=py_version\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8faed439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating model with name: pytorch-inference-2022-04-08-21-26-41-032\n",
      "Creating endpoint-config with name pytorch-inference-2022-04-08-21-26-41-333\n",
      "Creating endpoint with name pytorch-inference-2022-04-08-21-26-41-333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "instance_type = 'ml.g4dn.xlarge'\n",
    "uncompiled_predictor = sm_model.deploy(initial_instance_count=1, instance_type=instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5ac3ea26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import requests\n",
    "client = boto3.client('sagemaker-runtime', region_name=region)\n",
    "content_type = 'application/x-image'\n",
    "sample_img_url = \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"\n",
    "body = requests.get(sample_img_url).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cd35520a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.52 s Â± 9.97 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "rv = client.invoke_endpoint(EndpointName=uncompiled_predictor.endpoint_name, Body=body, ContentType=content_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5174f86",
   "metadata": {},
   "source": [
    "## Inference Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b03c78fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyT Version 1.7\n"
     ]
    }
   ],
   "source": [
    "# ML framework details\n",
    "framework = 'pytorch'\n",
    "# Note that only the framework major and minor version is supported for Neo compilation\n",
    "framework_version = '.'.join(torch.__version__.split('.')[:-1])\n",
    "# model name as standardized by model zoos or a similar open source model\n",
    "model_name = \"yolov5\"\n",
    "# ML model details\n",
    "ml_domain = \"COMPUTER_VISION\"\n",
    "ml_task = \"OBJECT_DETECTION\"\n",
    "\n",
    "print(\"PyT Version\", framework_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ab0ae691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.us-east-2.amazonaws.com/pytorch-inference:1.7-cpu-py3'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance_type = \"ml.c5.xlarge\"  # Note: you can use any GPU-based instance type here, this is just to get a GPU tagged image\n",
    "dlc_uri = image_uris.retrieve(\n",
    "    framework,\n",
    "    region,\n",
    "    version=framework_version,\n",
    "    py_version=\"py3\",\n",
    "    instance_type=instance_type,\n",
    "    image_scope=\"inference\",\n",
    ")\n",
    "dlc_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cedd199",
   "metadata": {},
   "source": [
    "### Create sample payload archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "fb53aaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload_archive_name = \"payload.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3e22e37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  ./sample-payload/  already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "## optional: download sample images\n",
    "SAMPLES_BUCKET = \"sagemaker-sample-files\"\n",
    "PREFIX = \"datasets/image/pets/\"\n",
    "payload_location = \"./sample-payload/\"\n",
    "\n",
    "if not os.path.exists(payload_location):\n",
    "    os.makedirs(payload_location)\n",
    "    print(\"Directory \", payload_location, \" Created \")\n",
    "else:\n",
    "    print(\"Directory \", payload_location, \" already exists\")\n",
    "\n",
    "sess.download_data(payload_location, SAMPLES_BUCKET, PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac9be89",
   "metadata": {},
   "source": [
    "### Tar the payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6c2d1907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boxer_dog.jpg\n",
      "british_blue_shorthair_cat.jpg\n",
      "english_cocker_spaniel_dog.jpg\n",
      "shiba_inu_dog.jpg\n"
     ]
    }
   ],
   "source": [
    "!cd ./sample-payload/ && tar czvf ../payload.tar.gz *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d530bdac",
   "metadata": {},
   "source": [
    "### Upload to S3\n",
    "\n",
    "Next, we'll upload the packaged payload examples (payload.tar.gz) that was created above to S3.  The S3 location will be used as input to our Inference Recommender job later in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "36240f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_payload_url = sess.upload_data(path=payload_archive_name, key_prefix=\"payload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e0c7cb",
   "metadata": {},
   "source": [
    "### Register model in Model Registry\n",
    "\n",
    "In order to use Inference Recommender, you must have a versioned model in SageMaker Model Registry.  To register a model in the Model Registry, you must have a model artifact packaged in a tarball and an inference container image.  Registering a model includes the following steps:\n",
    "\n",
    "\n",
    "1) **Create Model Group:** This is a one-time task per machine learning use case. A Model Group contains one or more versions of your packaged model. \n",
    "\n",
    "2) **Register Model Version/Package:** This task is performed for each new packaged model version. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd02d36b",
   "metadata": {},
   "source": [
    "### Create Model Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d589dffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelPackageGroup Arn : arn:aws:sagemaker:us-east-2:156991241640:model-package-group/pytorch-models-1649457428\n"
     ]
    }
   ],
   "source": [
    "model_package_group_name = \"{}-models-\".format(framework) + str(round(time.time()))\n",
    "model_package_group_description = \"{} models\".format(ml_task.lower())\n",
    "\n",
    "model_package_group_input_dict = {\n",
    "    \"ModelPackageGroupName\": model_package_group_name,\n",
    "    \"ModelPackageGroupDescription\": model_package_group_description,\n",
    "}\n",
    "\n",
    "create_model_package_group_response = sm_client.create_model_package_group(\n",
    "    **model_package_group_input_dict\n",
    ")\n",
    "print(\n",
    "    \"ModelPackageGroup Arn : {}\".format(create_model_package_group_response[\"ModelPackageGroupArn\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57566f3d",
   "metadata": {},
   "source": [
    "### Register Model Version/Package\n",
    "\n",
    "In this step, you'll register your pretrained model that was packaged in the prior steps as a new version in SageMaker Model Registry.  First, you'll configure the model package/version identifying which model package group this new model should be registered within as well as identify the initial approval status. You'll also identify the domain and task for your model.  These values were set earlier in the notebook \n",
    "where `ml_domain = 'COMPUTER_VISION'` and `ml_task = 'OBJECT_DETECTION'`\n",
    "\n",
    "*Note: ModelApprovalStatus is a configuration parameter that can be used in conjunction with SageMaker Projects to trigger automated deployment pipeline.*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1982f70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_description = \"{} {} inference recommender\".format(framework, model_name)\n",
    "\n",
    "model_approval_status = \"PendingManualApproval\"\n",
    "\n",
    "create_model_package_input_dict = {\n",
    "    \"ModelPackageGroupName\": model_package_group_name,\n",
    "    \"Domain\": ml_domain.upper(),\n",
    "    \"Task\": ml_task.upper(),\n",
    "    \"SamplePayloadUrl\": sample_payload_url,\n",
    "    \"ModelPackageDescription\": model_package_description,\n",
    "    \"ModelApprovalStatus\": model_approval_status,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d279b674",
   "metadata": {},
   "source": [
    "### Set up inference specification\n",
    "\n",
    "You'll now setup the inference specification configuration for your model version.  This contains information on how the model should be hosted.\n",
    "\n",
    "Inference Recommender expects a single input MIME type for sending requests. Learn more about [common inference data formats on SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-inference.html). This MIME type will be sent in the Content-Type header when invoking your endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "077c8a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mime_types = [\"application/octet-stream\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b353d41",
   "metadata": {},
   "source": [
    "If you specify a set of instance types below (i.e. non-empty list), then Inference Recommender will only support recommendations within the set of instances below. For this example, we provide a list of common instance types used for image classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4beee9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "supported_realtime_inference_types = [\n",
    "    \"ml.c5.xlarge\",\n",
    "    \"ml.inf1.xlarge\",\n",
    "    \"ml.g4dn.xlarge\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98094116",
   "metadata": {},
   "source": [
    "### Optional: Model optimization\n",
    "\n",
    "[Amazon SageMaker Neo](https://aws.amazon.com/sagemaker/neo) is a capability of SageMaker that automatically optimizes your ML models for any target instance type. With Neo, you donâ€™t need to set up third-party or framework-specific compiler software, or tune the model manually for optimizing inference performance. \n",
    "\n",
    "Inference Recommender compiles your model using SageMaker Neo if the `ModelInput` field is provided. To prepare the inputs for model compilation, specify the input shape for your trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "38a2fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input_configuration = \"[[1,3,640,640]]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "68477dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpackage_inference_specification = {\n",
    "    \"InferenceSpecification\": {\n",
    "        \"Containers\": [\n",
    "            {\n",
    "                \"Image\": dlc_uri,\n",
    "                \"Framework\": framework.upper(),\n",
    "                \"FrameworkVersion\": framework_version,\n",
    "                \"NearestModelName\": model_name,\n",
    "                \"ModelInput\": {\"DataInputConfig\": data_input_configuration},\n",
    "            }\n",
    "        ],\n",
    "        \"SupportedContentTypes\": input_mime_types,  # required, must be non-null\n",
    "        \"SupportedResponseMIMETypes\": [],\n",
    "        \"SupportedRealtimeInferenceInstanceTypes\": supported_realtime_inference_types,  # optional\n",
    "    }\n",
    "}\n",
    "\n",
    "# Specify the model data\n",
    "modelpackage_inference_specification[\"InferenceSpecification\"][\"Containers\"][0][\n",
    "    \"ModelDataUrl\"\n",
    "] = model_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2e1f5cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model_package_input_dict.update(modelpackage_inference_specification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "347fc1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelPackage Version ARN : arn:aws:sagemaker:us-east-2:156991241640:model-package/pytorch-models-1649457428/1\n"
     ]
    }
   ],
   "source": [
    "create_mode_package_response = sm_client.create_model_package(**create_model_package_input_dict)\n",
    "model_package_arn = create_mode_package_response[\"ModelPackageArn\"]\n",
    "print(\"ModelPackage Version ARN : {}\".format(model_package_arn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "b1ee2f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ModelPackageGroupName': 'pytorch-models-1649457428',\n",
       " 'ModelPackageVersion': 1,\n",
       " 'ModelPackageArn': 'arn:aws:sagemaker:us-east-2:156991241640:model-package/pytorch-models-1649457428/1',\n",
       " 'ModelPackageDescription': 'pytorch yolov5 inference recommender',\n",
       " 'CreationTime': datetime.datetime(2022, 4, 8, 22, 37, 18, 405000, tzinfo=tzlocal()),\n",
       " 'InferenceSpecification': {'Containers': [{'Image': '763104351884.dkr.ecr.us-east-2.amazonaws.com/pytorch-inference:1.7-cpu-py3',\n",
       "    'ImageDigest': 'sha256:1b6b7276ef97a34269479d73c180775b1fedd31bedaa083d406d7cce9ae633c4',\n",
       "    'ModelDataUrl': 's3://sagemaker-us-east-2-156991241640/model/model.tar.gz',\n",
       "    'ModelInput': {'DataInputConfig': '[[1,3,640,640]]'},\n",
       "    'Framework': 'PYTORCH',\n",
       "    'FrameworkVersion': '1.7',\n",
       "    'NearestModelName': 'yolov5'}],\n",
       "  'SupportedRealtimeInferenceInstanceTypes': ['ml.c5.xlarge',\n",
       "   'ml.inf1.xlarge',\n",
       "   'ml.g4dn.xlarge'],\n",
       "  'SupportedContentTypes': ['application/octet-stream'],\n",
       "  'SupportedResponseMIMETypes': []},\n",
       " 'ModelPackageStatus': 'Completed',\n",
       " 'ModelPackageStatusDetails': {'ValidationStatuses': [],\n",
       "  'ImageScanStatuses': []},\n",
       " 'CertifyForMarketplace': False,\n",
       " 'ModelApprovalStatus': 'PendingManualApproval',\n",
       " 'Domain': 'COMPUTER_VISION',\n",
       " 'Task': 'OBJECT_DETECTION',\n",
       " 'SamplePayloadUrl': 's3://sagemaker-us-east-2-156991241640/payload/payload.tar.gz',\n",
       " 'ResponseMetadata': {'RequestId': 'e2db47fb-7442-4ab4-b277-294ab1f1df84',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'e2db47fb-7442-4ab4-b277-294ab1f1df84',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1193',\n",
       "   'date': 'Fri, 08 Apr 2022 22:37:18 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.describe_model_package(ModelPackageName=model_package_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a53a14",
   "metadata": {},
   "source": [
    "### Create a SageMaker Inference Recommender Default Job\n",
    "\n",
    "Now with your model in Model Registry, you can kick off a 'Default' job to get instance recommendations. This only requires your `ModelPackageVersionArn` and comes back with recommendations within an hour. \n",
    "\n",
    "The output is a list of instance type recommendations with associated environment variables, cost, throughput and latency metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "f669ac18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'JobArn': 'arn:aws:sagemaker:us-east-2:156991241640:inference-recommendations-job/73515cfc-b78c-11ec-9bfd-06a8b5eb1f1a', 'ResponseMetadata': {'RequestId': '1fe573e2-4bc8-4fab-9e6a-c08b363c347b', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '1fe573e2-4bc8-4fab-9e6a-c08b363c347b', 'content-type': 'application/x-amz-json-1.1', 'content-length': '120', 'date': 'Fri, 08 Apr 2022 22:37:19 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import uuid\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "client = boto3.client(\"sagemaker\", region)\n",
    "\n",
    "role = get_execution_role()\n",
    "default_job = uuid.uuid1()\n",
    "default_response = client.create_inference_recommendations_job(\n",
    "    JobName=str(default_job),\n",
    "    JobDescription=\"Job Description\",\n",
    "    JobType=\"Default\",\n",
    "    RoleArn=role,\n",
    "    InputConfig={\"ModelPackageVersionArn\": model_package_arn},\n",
    ")\n",
    "\n",
    "print(default_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a158900",
   "metadata": {},
   "source": [
    "## 8. Instance Recommendation Results\n",
    "\n",
    "Each inference recommendation includes `InstanceType`, `InitialInstanceCount`, `EnvironmentParameters` which are tuned environment variable parameters for better performance. We also include performance and cost metrics such as `MaxInvocations`, `ModelLatency`, `CostPerHour` and `CostPerInference`. We believe these metrics will help you narrow down to a specific endpoint configuration that suits your use case. \n",
    "\n",
    "Example:   \n",
    "\n",
    "If your motivation is overall price-performance with an emphasis on throughput, then you should focus on `CostPerInference` metrics  \n",
    "If your motivation is a balance between latency and throughput, then you should focus on `ModelLatency` / `MaxInvocations` metrics\n",
    "\n",
    "| Metric | Description |\n",
    "| --- | --- |\n",
    "| ModelLatency | The interval of time taken by a model to respond as viewed from SageMaker. This interval includes the local communication times taken to send the request and to fetch the response from the container of a model and the time taken to complete the inference in the container. <br /> Units: Milliseconds |\n",
    "| MaximumInvocations | The maximum number of InvokeEndpoint requests sent to an endpoint per minute. <br /> Units: None |\n",
    "| CostPerHour | The estimated cost per hour for your real-time endpoint. <br /> Units: US Dollars |\n",
    "| CostPerInference | The estimated cost per inference for your real-time endpoint. <br /> Units: US Dollars |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d97817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In progress\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import pandas as pd\n",
    "\n",
    "finished = False\n",
    "while not finished:\n",
    "    inference_recommender_job = sm_client.describe_inference_recommendations_job(\n",
    "        JobName=str(default_job)\n",
    "    )\n",
    "    if inference_recommender_job[\"Status\"] in [\"COMPLETED\", \"STOPPED\", \"FAILED\"]:\n",
    "        finished = True\n",
    "    else:\n",
    "        print(\"In progress\")\n",
    "        time.sleep(300)\n",
    "\n",
    "if inference_recommender_job[\"Status\"] == \"FAILED\":\n",
    "    print(\"Inference recommender job failed \")\n",
    "    print(\"Failed Reason: {}\".format(inference_recommender_job[\"FailureReason\"]))\n",
    "else:\n",
    "    print(\"Inference recommender job completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53784e8d",
   "metadata": {},
   "source": [
    "### Detailing out the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1cb305",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {**x[\"EndpointConfiguration\"], **x[\"ModelConfiguration\"], **x[\"Metrics\"]}\n",
    "    for x in inference_recommender_job[\"InferenceRecommendations\"]\n",
    "]\n",
    "df = pd.DataFrame(data)\n",
    "df.drop(\"VariantName\", inplace=True, axis=1)\n",
    "pd.set_option(\"max_colwidth\", 400)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b416d296",
   "metadata": {},
   "source": [
    "### CloudWatch metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "91f2197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloudwatch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3e20c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_client = boto3.client(\"cloudwatch\", region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4a1818b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>InstanceType</th>\n",
       "      <th>MaximumInvocations</th>\n",
       "      <th>ModelLatency</th>\n",
       "      <th>CostPerHour</th>\n",
       "      <th>CostPerInference</th>\n",
       "      <th>EndpointName</th>\n",
       "      <th>VariantName</th>\n",
       "      <th>InitialCount</th>\n",
       "      <th>EnvParameters</th>\n",
       "      <th>StartTime</th>\n",
       "      <th>EndTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ml.g4dn.xlarge</td>\n",
       "      <td>41</td>\n",
       "      <td>4410</td>\n",
       "      <td>0.736000001430511474609375</td>\n",
       "      <td>0.00029918699874542653560638427734375</td>\n",
       "      <td>sm-epc-476aa900-8445-467e-8919-327b16270596</td>\n",
       "      <td>sm-epc-476aa900-8445-467e-8919-327b16270596</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>2022-04-08 21:42:51.425000+00:00</td>\n",
       "      <td>2022-04-08 22:19:25.090000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ml.g4dn.xlarge</td>\n",
       "      <td>42</td>\n",
       "      <td>4614</td>\n",
       "      <td>0.736000001430511474609375</td>\n",
       "      <td>0.00029206348699517548084259033203125</td>\n",
       "      <td>sm-epc-7d9aff47-fafd-462b-b2c6-ef8868be46fc</td>\n",
       "      <td>sm-epc-7d9aff47-fafd-462b-b2c6-ef8868be46fc</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>2022-04-08 21:42:51.425000+00:00</td>\n",
       "      <td>2022-04-08 22:19:25.090000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ml.inf1.6xlarge</td>\n",
       "      <td>1155</td>\n",
       "      <td>1356</td>\n",
       "      <td>1.41600000858306884765625</td>\n",
       "      <td>0.00002043290078290738165378570556640625</td>\n",
       "      <td>sm-epc-93bb16bb-7eb3-40e2-932c-26afae165533</td>\n",
       "      <td>sm-epc-93bb16bb-7eb3-40e2-932c-26afae165533</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'Key': 'NEURONCORE_GROUP_SIZES', 'ValueType': 'string', 'Value': '1'}]</td>\n",
       "      <td>2022-04-08 21:42:51.425000+00:00</td>\n",
       "      <td>2022-04-08 22:19:25.090000+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    InstanceType    MaximumInvocations  ModelLatency         CostPerHour                      CostPerInference                             EndpointName                                 VariantName                   InitialCount                               EnvParameters                                          StartTime                         EndTime             \n",
       "0   ml.g4dn.xlarge           41             4410      0.736000001430511474609375     0.00029918699874542653560638427734375  sm-epc-476aa900-8445-467e-8919-327b16270596  sm-epc-476aa900-8445-467e-8919-327b16270596        1                                                                             [] 2022-04-08 21:42:51.425000+00:00 2022-04-08 22:19:25.090000+00:00\n",
       "1   ml.g4dn.xlarge           42             4614      0.736000001430511474609375     0.00029206348699517548084259033203125  sm-epc-7d9aff47-fafd-462b-b2c6-ef8868be46fc  sm-epc-7d9aff47-fafd-462b-b2c6-ef8868be46fc        1                                                                             [] 2022-04-08 21:42:51.425000+00:00 2022-04-08 22:19:25.090000+00:00\n",
       "2  ml.inf1.6xlarge         1155             1356       1.41600000858306884765625  0.00002043290078290738165378570556640625  sm-epc-93bb16bb-7eb3-40e2-932c-26afae165533  sm-epc-93bb16bb-7eb3-40e2-932c-26afae165533        1       [{'Key': 'NEURONCORE_GROUP_SIZES', 'ValueType': 'string', 'Value': '1'}] 2022-04-08 21:42:51.425000+00:00 2022-04-08 22:19:25.090000+00:00"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_endpoint_metrics(sm_client, cw_client, region, str(default_job), include_plots=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
